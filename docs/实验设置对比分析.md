# W-DOE与DAL实验设置对比分析

## 文档目的

本文档为W-DOE (IEEE TPAMI 2025) 和DAL (NeurIPS 2023) 项目合并提供全面的实验设置对比分析，旨在确保在统一的实验框架下进行公平的性能对比。

**关键发现**: 两个项目的实验设置差异远超预期，不仅仅是auxiliary OOD数据集的选择不同，还包括评分计算方式、训练超参数、数据集支持范围等多个关键方面。

---

## 1. 数据集设置对比

### 1.1 Auxiliary OOD数据集 (训练时使用)

| 项目 | 数据集 | 规模 | 特点 | 加载方式 | 数据增强 |
|------|--------|------|------|----------|----------|
| **W-DOE** | TinyImageNet-200 | 200类，相对小规模 | 高质量，类别明确 | `dset.ImageFolder` | Resize→RandomCrop→Flip |
| **DAL** | 80 Million Tiny Images | 7930万张图片 | 超大规模，覆盖广泛 | 自定义二进制读取 | Binary→PIL→RandomCrop→Flip |

**关键差异**:
- **数据规模**: DAL使用的数据集比W-DOE大约400倍
- **数据质量**: W-DOE使用精选的200类数据，DAL使用海量无标签数据
- **CIFAR重叠处理**: DAL自动排除CIFAR重叠图片，W-DOE无此机制

### 1.2 ID数据集支持范围

| 项目 | CIFAR-10 | CIFAR-100 | 实现状态 |
|------|----------|-----------|----------|
| **W-DOE** | ✅ 支持 | ❌ 被注释 | CIFAR-100相关代码完全被注释 |
| **DAL** | ✅ 支持 | ✅ 支持 | 完整的双数据集支持 |

### 1.3 测试OOD数据集覆盖

| 测试数据集 | W-DOE | DAL | 说明 |
|------------|-------|-----|------|
| Textures (DTD) | ✅ | ✅ | 两个项目都支持 |
| SVHN | ✅ | ✅ | 两个项目都支持 |
| iSUN | ✅ | ✅ | 两个项目都支持 |
| CIFAR互测 | ✅ | ✅ | 两个项目都支持 |
| Places365 | ❌ 注释 | ✅ | W-DOE代码中被注释 |
| LSUN-C | ❌ 注释 | ✅ | W-DOE代码中被注释 |
| LSUN-R | ❌ 注释 | ✅ | W-DOE代码中被注释 |

**影响**: DAL的测试覆盖更全面，W-DOE缺少部分标准基准测试。

---

## 2. 训练配置对比

### 2.1 基础超参数对比

| 参数 | W-DOE默认值 | W-DOE推荐值 | DAL默认值 | DAL推荐值 |
|------|-------------|-------------|-----------|-----------|
| `epochs` | 10 | - | 50 | 50 |
| `learning_rate` | 0.01 | 0.01 | 0.01 | **0.07** |
| `batch_size` | 128 | 128 | 128 | 128 |
| `oe_batch_size` | 256 | 256 | 256 | 256 |
| `momentum` | 0.9 | 0.9 | 0.9 | 0.9 |
| `weight_decay` | 0.0005 | 0.0005 | 0.0005 | 0.0005 |

### 2.2 方法特定超参数

#### W-DOE特有参数
| 参数 | 默认值 | 说明 |
|------|--------|------|
| `warmup` | 5 | warmup阶段的epoch数 |
| `gamma` | 0 | W-DOE正则化强度 |

#### DAL特有参数  
| 参数 | 默认值 | CIFAR-10推荐 | CIFAR-100推荐 |
|------|--------|--------------|---------------|
| `gamma` | 1 | **10** | **10** |
| `beta` | 0.5 | **0.01** | **0.005** |
| `rho` | 0.01 | **10** | **10** |
| `iter` | 10 | 10 | 10 |
| `strength` | 0.01 | **1** | **1** |
| `warmup` | 0 | 0 | 0 |

### 2.3 优化器和调度器配置

| 组件 | W-DOE | DAL | 差异说明 |
|------|-------|-----|----------|
| **优化器** | SGD + Nesterov | SGD + Nesterov | 完全相同 |
| **学习率调度** | Cosine Annealing | Cosine Annealing | 基础算法相同 |
| **调度器配置** | 基于固定20个epoch | 基于`args.epochs` | **关键差异** |

**调度器代码对比**:
```python
# W-DOE: 固定20个epoch
scheduler = torch.optim.lr_scheduler.LambdaLR(
    optimizer, 
    lr_lambda=lambda step: cosine_annealing(step, 20 * len(train_loader_in), 1, 1e-6 / args.learning_rate)
)

# DAL: 基于实际训练epoch数
scheduler = torch.optim.lr_scheduler.LambdaLR(
    optimizer, 
    lr_lambda=lambda step: cosine_annealing(step, args.epochs * len(train_loader_in), 1, 1e-6 / args.learning_rate)
)
```

---

## 3. 评估配置对比

### 3.1 🚨 关键差异：评分计算方式

这是影响性能对比公平性的**最重要差异**：

```python
# W-DOE: 使用原始logits
def get_ood_scores(loader, in_dist=False):
    # ...
    output = net(data)
    # smax = to_np(F.softmax(output, dim=1))  # 被注释
    smax = to_np(output)  # 使用原始logits
    _score.append(-np.max(smax, axis=1))

# DAL: 使用softmax概率
def get_ood_scores(loader, in_dist=False):
    # ...
    output = net(data)
    smax = to_np(F.softmax(output, dim=1))  # 使用softmax
    _score.append(-np.max(smax, axis=1))
```

**影响分析**:
- 原始logits和softmax概率的数值范围完全不同
- 这会导致OOD检测性能的显著差异
- **必须统一评分方式才能进行公平对比**

### 3.2 评估指标一致性

✅ **完全一致**: 两个项目都使用相同的OOD检测标准指标
- **FPR95**: False Positive Rate at 95% True Positive Rate
- **AUROC**: Area Under ROC Curve  
- **AUPR**: Area Under Precision-Recall Curve

### 3.3 评估协议差异

| 方面 | W-DOE | DAL |
|------|-------|-----|
| **重复实验** | 支持多次运行平均 | 单次运行 |
| **测试集采样** | `ood_num_examples = len(test_data) // 5` | 相同 |
| **结果输出** | 可选择是否打印详细结果 | 强制打印详细结果 |

---

## 4. 模型架构一致性确认

✅ **完全一致**: 两个项目使用相同的模型架构

| 组件 | 配置 | 确认 |
|------|------|------|
| **模型** | WideResNet-40-2 | ✅ 相同 |
| **层数** | 40层 | ✅ 相同 |
| **宽度系数** | 2 | ✅ 相同 |
| **Dropout** | 0.3 | ✅ 相同 |
| **预训练模型** | 相同的预训练权重 | ✅ 相同 |

### 数据预处理一致性确认

✅ **完全一致**: 两个项目使用相同的数据预处理
```python
# 标准化参数 (完全相同)
mean = [0.4914, 0.4822, 0.4465]  # [125.3/255, 123.0/255, 113.9/255]
std = [0.2471, 0.2435, 0.2616]   # [63.0/255, 62.1/255, 66.7/255]

# Transform管道 (完全相同)
train_transform = trn.Compose([
    trn.RandomHorizontalFlip(), 
    trn.RandomCrop(32, padding=4),
    trn.ToTensor(), 
    trn.Normalize(mean, std)
])
```

---

## 5. Merge设置建议

### 5.1 统一实验设置推荐

为确保公平对比，建议采用以下统一设置：

#### A. 基础配置统一
```python
# 推荐的统一基础配置
UNIFIED_CONFIG = {
    'epochs': 100,  # 足够的训练时间
    'learning_rate': 0.01,  # W-DOE的设置作为baseline
    'batch_size': 128,
    'oe_batch_size': 256,
    'momentum': 0.9,
    'weight_decay': 0.0005,
    'warmup': 5,  # 采用W-DOE的warmup机制
}
```

#### B. 评分计算方式统一
🚨 **必须统一**: 建议采用W-DOE的原始logits方式
```python
# 推荐的统一评分函数
def unified_get_ood_scores(loader, in_dist=False):
    # ...
    output = net(data)
    smax = to_np(output)  # 使用原始logits，保持一致
    _score.append(-np.max(smax, axis=1))
```

#### C. 测试集覆盖统一
- 启用W-DOE中被注释的测试集 (Places365, LSUN等)
- 确保两种方法在相同的测试基准上评估

### 5.2 方法特定配置

每种方法保持其特有的核心超参数：

#### W-DOE保持
```python
W_DOE_SPECIFIC = {
    'gamma': 0.5,  # W-DOE正则化强度
    'warmup': 5,   # warmup机制
}
```

#### DAL保持
```python
DAL_SPECIFIC = {
    'gamma': 10,
    'beta': 0.01,   # CIFAR-10
    'rho': 10,
    'iter': 10,
    'strength': 1,
}
```

---

## 6. 公平对比指南

### 6.1 对比实验设计原则

1. **固定变量**: 除auxiliary OOD数据集外，所有其他设置必须相同
2. **统一评分**: 必须使用相同的评分计算方式
3. **相同基准**: 在相同的测试数据集上评估
4. **足够训练**: 确保两种方法都得到充分训练

### 6.2 关键对齐检查清单

- [ ] **评分方式统一**: 确认使用相同的logits/softmax评分
- [ ] **测试集完整**: 确认所有标准OOD测试集都被包含
- [ ] **超参数合理**: 确认关键超参数设置合理
- [ ] **预处理一致**: 确认数据预处理完全相同
- [ ] **模型架构一致**: 确认使用相同的WideResNet配置

### 6.3 性能差异归因分析

在统一设置下，如果仍存在性能差异，可能的原因：

1. **Auxiliary数据质量**: TinyImageNet-200 vs 80M Tiny Images
2. **数据规模效应**: 大规模数据 vs 精选数据的影响  
3. **算法本质差异**: 参数空间增强 vs 特征空间增强
4. **训练稳定性**: 不同方法的收敛特性差异

---

## 7. 技术实现注意事项

### 7.1 代码合并兼容性

#### 需要注意的差异
1. **评分函数**: 确保统一`get_ood_scores`实现
2. **数据加载**: 处理不同auxiliary数据集的加载逻辑
3. **超参数管理**: 建立清晰的配置管理系统
4. **路径配置**: 修复W-DOE中的路径错误问题

#### 建议的代码结构
```
unified_framework/
├── config/
│   ├── wdoe_config.py      # W-DOE特定配置
│   ├── dal_config.py       # DAL特定配置
│   └── unified_config.py   # 统一基础配置
├── data/
│   ├── auxiliary_loaders.py # 统一的auxiliary数据加载
│   └── test_loaders.py      # 统一的测试数据加载
├── models/
│   └── wrn.py              # 共用模型定义
├── methods/
│   ├── wdoe.py            # W-DOE训练逻辑
│   └── dal.py             # DAL训练逻辑
└── evaluation/
    └── unified_eval.py     # 统一评估框架
```

### 7.2 配置文件设计

建议使用YAML配置文件管理不同实验设置：

```yaml
# config/unified_experiment.yaml
base:
  epochs: 100
  learning_rate: 0.01
  batch_size: 128
  evaluation_mode: 'logits'  # 统一使用logits评分

methods:
  wdoe:
    gamma: 0.5
    warmup: 5
    auxiliary_data: 'tinyimagenet200'
  
  dal:
    gamma: 10
    beta: 0.01
    rho: 10
    auxiliary_data: '80m_tinyimages'

datasets:
  test_ood: ['dtd', 'svhn', 'isun', 'cifar_cross', 'places365', 'lsun_c', 'lsun_r']
```

### 7.3 测试脚本开发

```python
# 建议的统一实验脚本
def run_unified_experiment(method_name, config_path):
    config = load_config(config_path)
    
    # 加载统一的基础设置
    base_config = config['base']
    method_config = config['methods'][method_name]
    
    # 确保评估方式统一
    assert base_config['evaluation_mode'] == 'logits'
    
    # 运行实验
    results = train_and_evaluate(
        method=method_name,
        config={**base_config, **method_config}
    )
    
    return results
```

---

## 8. 结论与建议

### 8.1 主要发现总结

1. **设置差异显著**: 两项目的差异远超auxiliary数据集选择
2. **评分方式关键**: logits vs softmax是影响公平性的核心问题  
3. **测试覆盖不一**: W-DOE的测试基准不够全面
4. **超参数差异大**: 训练配置存在显著差异

### 8.2 Merge优先级建议

**高优先级** (必须解决):
1. 统一评分计算方式
2. 完善W-DOE的测试数据集覆盖
3. 建立统一的配置管理系统

**中优先级** (建议解决):  
1. 统一基础超参数设置
2. 修复代码中的路径错误
3. 建立自动化实验框架

**低优先级** (可选):
1. 代码结构重构优化
2. 添加更多评估指标
3. 性能可视化工具

### 8.3 预期效果

通过实施本文档的建议，可以实现：
- ✅ 在统一框架下公平对比两种方法
- ✅ 消除非本质因素对性能比较的影响  
- ✅ 建立可重现的实验环境
- ✅ 为后续研究提供标准化基准

---

*文档创建日期: 2025-01-29*  
*分析工具: Claude Code*  
*用途: W-DOE与DAL项目合并技术指导*